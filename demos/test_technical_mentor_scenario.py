#!/usr/bin/env python3
"""
Comprehensive test scenario for technical_mentor persona.
Tests long conversation context and examines each module's output.
"""
import asyncio
import json
from dotenv import load_dotenv

from agenesis.core import Agent
from agenesis.persona import PersonaContext

# Load environment variables
load_dotenv()


async def test_technical_mentor_comprehensive():
    """Test technical mentor persona with detailed output inspection"""
    print("ğŸ§  Technical Mentor Persona - Comprehensive Test\n")
    
    # Create agent with technical mentor persona
    agent = Agent(profile="tech_mentor_test", persona="technical_mentor")
    
    print("âœ… Agent initialized:")
    profile_info = agent.get_profile_info()
    print(f"   Profile: {agent.profile}")
    print(f"   Persona: {profile_info['persona']['name']}")
    print(f"   Description: {profile_info['persona']['description']}")
    print(f"   Persistent memory: {profile_info['has_persistent_memory']}")
    print()
    
    # Long conversation scenario - technical debugging session
    conversation = [
        # 1. Initial problem description
        "Hi, I'm working on a Python web application and I'm experiencing some performance issues",
        
        # 2. More specific problem
        "The API response times are really slow, sometimes taking 5-10 seconds for simple queries",
        
        # 3. Technical details
        "I'm using Flask with SQLAlchemy ORM, PostgreSQL database, and the app is deployed on AWS EC2",
        
        # 4. Error mention (should trigger priority signals)
        "I'm also getting occasional timeout errors when the database queries take too long",
        
        # 5. Code question  
        "Here's my query code: User.query.filter_by(active=True).all() - is this efficient?",
        
        # 6. Architecture question
        "Should I be using database connection pooling? How do I optimize this?",
        
        # 7. Debugging approach
        "What's the best way to debug and profile these performance bottlenecks?",
        
        # 8. Scaling concerns (priority signal)
        "The app needs to scale to handle 1000+ concurrent users. What should I consider?"
    ]
    
    print("ğŸ“ Testing conversation with detailed module inspection:\n")
    
    for i, user_input in enumerate(conversation, 1):
        print(f"{'='*80}")
        print(f"ğŸ”„ INTERACTION #{i}")
        print(f"ğŸ‘¤ User: {user_input}")
        print(f"{'='*80}")
        
        # === 1. PERSONA CONTEXT GENERATION ===
        print("\n1ï¸âƒ£ PERSONA CONTEXT GENERATION:")
        persona_context = agent.persona.create_context(user_input)
        print(f"   ğŸ¯ Focus Areas: {persona_context.focus_areas}")
        print(f"   ğŸš¨ Priority Signals: {persona_context.priority_signals}")
        print(f"   ğŸ“ˆ Relevance Boosts: {persona_context.relevance_boosts}")
        print(f"   ğŸ§  Reasoning Approach: {persona_context.reasoning_approach}")
        print(f"   ğŸ“Š Detail Level: {persona_context.detail_level}")
        print(f"   ğŸ’¡ System Additions: {len(persona_context.system_additions)} items")
        
        # Check priority signal detection
        signals_detected = [signal for signal in persona_context.priority_signals 
                          if signal.lower() in user_input.lower()]
        if signals_detected:
            print(f"   ğŸš¨ PRIORITY SIGNALS DETECTED: {signals_detected}")
            print(f"   ğŸ“ˆ Detail Level Auto-Adjusted: {persona_context.detail_level}")
        
        # === 2. PERCEPTION MODULE ===
        print("\n2ï¸âƒ£ PERCEPTION MODULE:")
        perception_result = agent.perception.process(user_input, persona_context)
        print(f"   ğŸ“ Content Length: {len(perception_result.content)} chars")
        print(f"   ğŸ·ï¸  Input Type: {perception_result.input_type}")
        print(f"   ğŸ“Š Features Extracted:")
        for key, value in perception_result.features.items():
            print(f"      - {key}: {value}")
        
        # Check if persona influenced perception
        print(f"   ğŸ­ Persona Context Provided: {persona_context is not None}")
        
        # === 3. MEMORY STORAGE ===
        print("\n3ï¸âƒ£ MEMORY STORAGE:")
        memory_context = {"persona_context": persona_context.to_dict()} if persona_context else None
        
        # Store in all memory systems
        immediate_id = agent.immediate_memory.store(perception_result, memory_context)
        working_id = agent.working_memory.store(perception_result, memory_context)
        if agent.has_persistent_memory:
            persistent_id = agent.persistent_memory.store(perception_result, memory_context)
        
        print(f"   ğŸ’¾ Immediate Memory ID: {immediate_id}")
        print(f"   ğŸ’¾ Working Memory ID: {working_id}")
        if agent.has_persistent_memory:
            print(f"   ğŸ’¾ Persistent Memory ID: {persistent_id}")
        
        # Check memory contents
        current_record = agent.immediate_memory.get_current()
        print(f"   ğŸ“„ Current Focus: {current_record.perception_result.content[:50]}...")
        print(f"   ğŸ­ Persona Context Stored: {'persona_context' in current_record.context}")
        
        working_memories = agent.working_memory.get_recent(3)
        print(f"   ğŸ“š Working Memory Size: {len(working_memories)} items")
        
        # === 4. COGNITION MODULE ===
        print("\n4ï¸âƒ£ COGNITION MODULE:")
        cognition_result = await agent.cognition.process(
            agent.immediate_memory, agent.working_memory, persona_context
        )
        
        print(f"   ğŸ§  Intent: {cognition_result.intent}")
        print(f"   ğŸ¯ Confidence: {cognition_result.confidence}")
        print(f"   ğŸ“ Summary: {cognition_result.summary[:100]}...")
        print(f"   ğŸ”§ Context Type: {cognition_result.context_type}")
        print(f"   ğŸ“Š Persistence Score: {cognition_result.persistence_score}")
        print(f"   ğŸ§  Reasoning: {cognition_result.reasoning[:100]}...")
        print(f"   ğŸ“š Relevant Memories: {len(cognition_result.relevant_memories)} items")
        print(f"   ğŸ­ Persona Context Used: {persona_context is not None}")
        
        # Check if persona influenced cognition
        if persona_context and persona_context.reasoning_approach:
            print(f"   ğŸ§­ Persona Reasoning: {persona_context.reasoning_approach}")
        
        # === 5. ACTION MODULE ===
        print("\n5ï¸âƒ£ ACTION MODULE:")
        action_result = await agent.action.generate_response(cognition_result, persona_context)
        
        print(f"   ğŸ“¤ Response Length: {len(action_result.response_text)} chars")
        print(f"   ğŸ¯ Action Confidence: {action_result.confidence}")
        print(f"   ğŸ­ Persona Context Used: {persona_context is not None}")
        if action_result.internal_reasoning:
            print(f"   ğŸ”§ Internal Reasoning: {action_result.internal_reasoning[:100]}...")
        
        # === 6. FINAL RESPONSE ===
        print("\n6ï¸âƒ£ FINAL RESPONSE:")
        print(f"ğŸ¤– Agent: {action_result.response_text}")
        
        # === 7. MEMORY ANALYSIS ===
        print("\n7ï¸âƒ£ MEMORY ANALYSIS:")
        session_context = agent.get_session_context(5)
        print(f"   ğŸ“Š Session Context Items: {len(session_context)}")
        
        # Check for persona influence accumulation
        if len(working_memories) > 1:
            print("   ğŸ”„ Conversation History:")
            for idx, memory in enumerate(working_memories[-3:], 1):
                snippet = memory.perception_result.content[:30]
                has_persona = 'persona_context' in memory.context
                print(f"      {idx}. {snippet}... (persona: {has_persona})")
        
        print(f"\n{'='*80}\n")
        
        # Brief pause between interactions
        await asyncio.sleep(0.1)
    
    # === FINAL ANALYSIS ===
    print("ğŸ“Š FINAL SESSION ANALYSIS:")
    final_profile = agent.get_profile_info()
    print(f"   ğŸ‘¤ Profile: {final_profile['profile']}")
    print(f"   ğŸ­ Persona Active: {final_profile['persona']['active']}")
    print(f"   ğŸ“š Session Size: {final_profile['session_size']} interactions")
    print(f"   ğŸ¯ Current Focus: {final_profile['current_focus']}")
    
    # Memory distribution analysis
    all_working_memories = agent.working_memory.get_recent(10)
    persona_memories = [m for m in all_working_memories if 'persona_context' in m.context]
    print(f"   ğŸ’¾ Total Working Memories: {len(all_working_memories)}")
    print(f"   ğŸ­ Memories with Persona Context: {len(persona_memories)}")
    
    # Check persona consistency
    if persona_memories:
        sample_persona_data = persona_memories[0].context['persona_context']
        print(f"   ğŸ“‹ Persona Focus Areas: {sample_persona_data.get('focus_areas', [])}")
        print(f"   ğŸš¨ Persona Priority Signals: {sample_persona_data.get('priority_signals', [])}")
        print(f"   ğŸ§  Persona Reasoning: {sample_persona_data.get('reasoning_approach')}")
    
    print("\nâœ… Technical Mentor Persona Test Complete!")


async def test_persona_context_details():
    """Detailed examination of persona context generation"""
    print("\nğŸ”¬ DETAILED PERSONA CONTEXT ANALYSIS\n")
    
    agent = Agent(profile="context_test", persona="technical_mentor")
    
    test_inputs = [
        ("Normal query", "How do I connect to a database?"),
        ("Performance issue", "My app is running really slow"),
        ("Error case", "I'm getting database timeout errors"),
        ("Architecture question", "Should I use microservices for scaling?"),
        ("Debug request", "How do I debug memory leaks in Python?"),
        ("Multiple signals", "I have performance errors that need debugging for scale")
    ]
    
    for label, input_text in test_inputs:
        print(f"ğŸ“ {label}: '{input_text}'")
        context = agent.persona.create_context(input_text)
        
        print(f"   ğŸ¯ Focus Areas: {context.focus_areas}")
        print(f"   ğŸš¨ Priority Signals: {context.priority_signals}")
        print(f"   ğŸ“Š Detail Level: {context.detail_level}")
        print(f"   ğŸ§  Reasoning: {context.reasoning_approach}")
        
        # Check signal detection
        signals_found = [sig for sig in context.priority_signals 
                        if sig.lower() in input_text.lower()]
        print(f"   âœ… Signals Detected: {signals_found}")
        print(f"   ğŸ“ˆ Context Has Content: {context.has_content()}")
        print()


if __name__ == "__main__":
    print("ğŸ§ª Starting Comprehensive Technical Mentor Persona Test\n")
    asyncio.run(test_technical_mentor_comprehensive())
    asyncio.run(test_persona_context_details())